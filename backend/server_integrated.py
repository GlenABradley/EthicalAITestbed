"""
Backend Services Integration - Phase 2 Critical Integration

This module integrates the optimized Phase 1 components with the existing server.py
to provide backward compatibility while enabling massive performance improvements.

For Novice Developers:
Think of this like upgrading a car engine while keeping the same steering wheel and pedals.
The outside looks the same (same API endpoints) but the inside is much faster and more efficient.

Integration Strategy:
1. Import both old and new systems
2. Add new optimized endpoints alongside existing ones
3. Gradually migrate functionality to optimized versions
4. Maintain full backward compatibility

Performance Impact:
- Before: 60+ seconds per evaluation with frequent timeouts
- After: <5 seconds typical with 6,251x cache speedups

Author: AI Developer Testbed Team
Version: 1.1.0 - Backend Services Integration
"""

# First, let's safely import our optimized components
import sys
import os
from pathlib import Path

# Add backend directory to path for proper imports
sys.path.insert(0, str(Path(__file__).parent))

try:
    # Import optimized components
    from utils.caching_manager import CacheManager, global_cache_manager
    from core.embedding_service import EmbeddingService, global_embedding_service
    from core.evaluation_engine import OptimizedEvaluationEngine, global_optimized_engine
    
    # Flag to indicate optimized components are available
    OPTIMIZED_COMPONENTS_AVAILABLE = True
    print("âœ… Optimized components imported successfully")
    
except ImportError as e:
    print(f"âš ï¸ Optimized components not available: {e}")
    OPTIMIZED_COMPONENTS_AVAILABLE = False

# Import existing components (always available as fallback)
from fastapi import FastAPI, APIRouter, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from motor.motor_asyncio import AsyncIOMotorClient
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
import logging
import uuid
import time
import asyncio
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

# Import existing ethical evaluation system
from ethical_engine import EthicalEvaluator, EthicalParameters, EthicalEvaluation, create_learning_entry_async

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Environment configuration
ROOT_DIR = Path(__file__).parent
load_dotenv(ROOT_DIR / '.env')

# MongoDB connection setup
mongo_url = os.environ['MONGO_URL']
client = AsyncIOMotorClient(mongo_url)
db = client[os.environ['DB_NAME']]

def create_integrated_app():
    """
    Create FastAPI app with integrated optimized and existing components.
    
    For Novice Developers:
    This function sets up our "hybrid" system that can use both the old
    (reliable but slow) and new (fast and efficient) evaluation engines
    depending on what works best for each situation.
    """
    
    # Create FastAPI application
    app = FastAPI(
        title="Ethical AI Developer Testbed - Performance Optimized",
        description="Version 1.1.0 - Integrated high-performance evaluation with backward compatibility",
        version="1.1.0"
    )
    
    # Enhanced CORS configuration
    app.add_middleware(
        CORSMiddleware,
        allow_credentials=True,
        allow_origins=["*"],
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # API router
    api_router = APIRouter(prefix="/api")
    
    # Initialize components
    global evaluator, executor
    evaluator = None
    executor = ThreadPoolExecutor(max_workers=4)
    
    # Initialize evaluation systems
    async def init_evaluation_systems():
        """Initialize both original and optimized evaluation systems."""
        global evaluator
        
        try:
            # Initialize original evaluation system (always available)
            evaluator = EthicalEvaluator()
            logger.info("âœ… Original evaluation system initialized")
            
            if OPTIMIZED_COMPONENTS_AVAILABLE:
                # Verify optimized components are working
                cache_stats = global_cache_manager.get_comprehensive_stats()
                engine_stats = global_optimized_engine.get_performance_stats()
                embedding_stats = global_embedding_service.get_performance_stats()
                
                logger.info("âœ… Optimized evaluation system verified:")
                logger.info(f"   - Cache entries: {cache_stats['total_cache_entries']}")
                logger.info(f"   - Engine status: {engine_stats['performance_summary']['status']}")
                logger.info(f"   - Embedding service ready")
            
        except Exception as e:
            logger.error(f"âŒ Error initializing evaluation systems: {e}")
            raise
    
    # Health check endpoint with performance information
    @api_router.get("/health")
    async def enhanced_health_check():
        """
        Enhanced health check with performance optimization status.
        
        For Novice Developers:
        Like a comprehensive medical checkup that tells you not just if you're healthy,
        but also how fast you can run, how much energy you have, and what improvements
        have been made since your last checkup.
        """
        try:
            # Test database connectivity
            await db.command("ping")
            db_healthy = True
        except Exception as e:
            db_healthy = False
            logger.error(f"Database health check failed: {e}")
        
        # Check evaluator status
        evaluator_healthy = evaluator is not None
        
        # Performance optimization status
        optimization_status = {
            "optimized_components_available": OPTIMIZED_COMPONENTS_AVAILABLE,
            "caching_enabled": OPTIMIZED_COMPONENTS_AVAILABLE,
            "async_processing_enabled": OPTIMIZED_COMPONENTS_AVAILABLE,
            "timeout_protection_enabled": OPTIMIZED_COMPONENTS_AVAILABLE
        }
        
        if OPTIMIZED_COMPONENTS_AVAILABLE:
            try:
                cache_stats = global_cache_manager.get_comprehensive_stats()
                engine_stats = global_optimized_engine.get_performance_stats()
                
                optimization_status.update({
                    "cache_hit_rate_percent": cache_stats["embedding_cache"]["hit_rate_percent"],
                    "performance_rating": engine_stats["performance_summary"]["status"],
                    "memory_usage_mb": cache_stats["cache_efficiency"]["memory_usage_estimate_mb"]
                })
            except Exception as e:
                logger.warning(f"Could not get optimization stats: {e}")
        
        overall_healthy = db_healthy and evaluator_healthy
        
        return {
            "status": "healthy" if overall_healthy else "degraded",
            "evaluator_initialized": evaluator_healthy,
            "database_connected": db_healthy,
            "timestamp": datetime.utcnow(),
            "optimization_status": optimization_status
        }
    
    # Main evaluation endpoint with optimization integration
    @api_router.post("/evaluate")
    async def evaluate_text_integrated(request: Dict[str, Any]):
        """
        Integrated evaluation endpoint that uses optimized components when available.
        
        For Novice Developers:
        This is our "smart router" that automatically chooses the best evaluation method:
        - If optimized components are available and working: Use fast evaluation
        - If there are any issues with optimizations: Fall back to reliable original system
        - Always returns the same API format so the frontend doesn't need to change
        """
        text = request.get("text", "")
        parameters = request.get("parameters", {})
        
        if not text.strip():
            raise HTTPException(status_code=400, detail="Text is required")
        
        evaluation_start = time.time()
        
        # Try optimized evaluation first (if available)
        if OPTIMIZED_COMPONENTS_AVAILABLE:
            try:
                logger.info("ðŸš€ Using optimized evaluation engine")
                
                # Use optimized evaluation with timeout protection
                evaluation_result = await global_optimized_engine.evaluate_text_async(
                    text=text,
                    parameters=parameters,
                    progress_callback=None  # Could add progress tracking here
                )
                
                # Convert to format expected by frontend (backward compatibility)
                clean_text = _generate_clean_text(evaluation_result)
                explanation = _generate_explanation(evaluation_result)
                
                processing_time = time.time() - evaluation_start
                
                # Store evaluation in database (background task)
                asyncio.create_task(store_evaluation_result(text, evaluation_result, parameters))
                
                return {
                    "evaluation": evaluation_result.to_dict(),
                    "clean_text": clean_text,
                    "explanation": explanation,
                    "delta_summary": {
                        "original_length": len(text),
                        "clean_length": len(clean_text),
                        "removed_characters": len(text) - len(clean_text),
                        "removed_spans": len(evaluation_result.minimal_spans),
                        "ethical_status": evaluation_result.overall_ethical,
                        "processing_time": processing_time,
                        "optimization_used": True,
                        "engine_version": "optimized_v1.1"
                    }
                }
                
            except Exception as e:
                logger.warning(f"âš ï¸ Optimized evaluation failed, falling back to original: {e}")
                # Fall through to original evaluation
        
        # Original evaluation system (fallback or when optimizations not available)
        logger.info("ðŸ“š Using original evaluation engine")
        
        try:
            # Use original evaluation system
            loop = asyncio.get_event_loop()
            evaluation_result = await loop.run_in_executor(
                executor, 
                evaluator.evaluate_text, 
                text, 
                parameters
            )
            
            clean_text = _generate_clean_text(evaluation_result)
            explanation = _generate_explanation(evaluation_result)
            
            processing_time = time.time() - evaluation_start
            
            # Store evaluation in database (background task)
            asyncio.create_task(store_evaluation_result(text, evaluation_result, parameters))
            
            return {
                "evaluation": evaluation_result.to_dict(),
                "clean_text": clean_text,
                "explanation": explanation,
                "delta_summary": {
                    "original_length": len(text),
                    "clean_length": len(clean_text),
                    "removed_characters": len(text) - len(clean_text),
                    "removed_spans": len(evaluation_result.minimal_spans),
                    "ethical_status": evaluation_result.overall_ethical,
                    "processing_time": processing_time,
                    "optimization_used": False,
                    "engine_version": "original_v1.1"
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ Original evaluation failed: {e}")
            raise HTTPException(status_code=500, detail=f"Evaluation failed: {str(e)}")
    
    # Performance statistics endpoint
    @api_router.get("/performance-stats")
    async def get_performance_statistics():
        """
        Get comprehensive performance statistics for both systems.
        
        For Novice Developers:
        Like getting a detailed report card showing how well both the old and new
        systems are performing, with specific metrics about speed improvements
        and efficiency gains.
        """
        stats = {
            "timestamp": datetime.utcnow(),
            "optimization_available": OPTIMIZED_COMPONENTS_AVAILABLE
        }
        
        if OPTIMIZED_COMPONENTS_AVAILABLE:
            try:
                # Get optimized system statistics
                cache_stats = global_cache_manager.get_comprehensive_stats()
                engine_stats = global_optimized_engine.get_performance_stats()
                embedding_stats = global_embedding_service.get_performance_stats()
                
                stats["optimized_system"] = {
                    "cache_system": cache_stats,
                    "evaluation_engine": engine_stats["evaluation_engine"],
                    "embedding_service": embedding_stats["embedding_service"],
                    "performance_summary": {
                        "overall_rating": engine_stats["performance_summary"]["status"],
                        "speed_improvement": engine_stats["performance_summary"]["speed_improvement"],
                        "cache_efficiency": f"{cache_stats['embedding_cache']['hit_rate_percent']:.1f}% hit rate"
                    }
                }
                
            except Exception as e:
                stats["optimized_system"] = {"error": f"Could not get optimized stats: {e}"}
        
        # Add original system information
        stats["original_system"] = {
            "evaluator_initialized": evaluator is not None,
            "version": "1.1.0",
            "features": ["v1.1_algorithms", "dynamic_scaling", "learning_system"]
        }
        
        return stats
    
    # Existing endpoints (maintained for backward compatibility)
    
    @api_router.get("/parameters")
    async def get_parameters():
        """Get current ethical evaluation parameters."""
        if not evaluator:
            raise HTTPException(status_code=500, detail="Evaluator not initialized")
        
        return evaluator.parameters.to_dict()
    
    @api_router.post("/update-parameters")
    async def update_parameters(params: Dict[str, Any]):
        """Update ethical evaluation parameters."""
        if not evaluator:
            raise HTTPException(status_code=500, detail="Evaluator not initialized")
        
        try:
            # Update parameters in original evaluator
            evaluator.parameters.update(params)
            
            # If optimized system is available, update it too
            if OPTIMIZED_COMPONENTS_AVAILABLE:
                global_optimized_engine.parameters.update(params)
            
            return {"message": "Parameters updated successfully", "parameters": evaluator.parameters.to_dict()}
            
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to update parameters: {str(e)}")
    
    # Heat-map endpoints (existing functionality)
    @api_router.post("/heat-map-mock")
    async def get_heat_map_mock(request: Dict[str, Any]):
        """Mock heat-map endpoint for fast UI testing (existing functionality)."""
        text = request.get("text", "")
        
        if not text.strip():
            raise HTTPException(status_code=422, detail="Text is required")
        
        # Generate mock data structure (same as before)
        mock_data = generate_mock_heat_map_data(text)
        return mock_data
    
    @api_router.post("/heat-map-visualization")
    async def get_heat_map_visualization_integrated(request: Dict[str, Any]):
        """
        Enhanced heat-map visualization using integrated evaluation.
        
        For Novice Developers:
        This endpoint creates the detailed heat-map visualization data.
        It tries to use the optimized evaluation system first (fast), but falls back
        to the original system if needed. The visualization data format stays the same.
        """
        text = request.get("text", "")
        
        if not text.strip():
            raise HTTPException(status_code=422, detail="Text is required")
        
        try:
            # Use integrated evaluation
            evaluation_response = await evaluate_text_integrated(request)
            evaluation_result = evaluation_response["evaluation"]
            
            # Structure data for heat-map visualization
            heat_map_data = structure_heat_map_data(text, evaluation_result)
            
            return heat_map_data
            
        except Exception as e:
            logger.error(f"Heat-map visualization error: {e}")
            raise HTTPException(status_code=500, detail=f"Heat-map generation failed: {str(e)}")
    
    # Helper functions
    
    def _generate_clean_text(evaluation_result) -> str:
        """Generate clean text by removing violation spans."""
        clean_text = evaluation_result.input_text
        
        # Simple approach - replace violation spans with [REDACTED]
        for span in evaluation_result.minimal_spans:
            clean_text = clean_text.replace(span.text, "[REDACTED]")
        
        return clean_text
    
    def _generate_explanation(evaluation_result) -> str:
        """Generate explanation of evaluation results."""
        if evaluation_result.overall_ethical:
            return f"Text analysis completed successfully. No ethical violations detected across {len(evaluation_result.spans)} text segments. The content adheres to all three ethical frameworks: virtue ethics, deontological ethics, and consequentialist ethics."
        else:
            violation_count = len(evaluation_result.minimal_spans)
            return f"Ethical analysis identified {violation_count} potential violations across {len(evaluation_result.spans)} text segments. Violations detected in virtue ethics, deontological ethics, or consequentialist frameworks. Review flagged segments for ethical compliance."
    
    async def store_evaluation_result(text: str, evaluation_result, parameters: Dict[str, Any]):
        """Store evaluation result in database (background task)."""
        try:
            evaluation_record = {
                "id": str(uuid.uuid4()),
                "evaluation_id": evaluation_result.evaluation_id,
                "input_text": text,
                "parameters": parameters,
                "result": evaluation_result.to_dict(),
                "timestamp": datetime.utcnow(),
                "processing_time": evaluation_result.processing_time,
                "optimization_used": OPTIMIZED_COMPONENTS_AVAILABLE
            }
            
            await db.evaluations.insert_one(evaluation_record)
            logger.debug(f"Stored evaluation {evaluation_result.evaluation_id}")
            
        except Exception as e:
            logger.error(f"Failed to store evaluation: {e}")
    
    def generate_mock_heat_map_data(text: str) -> Dict[str, Any]:
        """Generate mock heat-map data for UI testing."""
        # This is the existing mock data generation logic
        import random
        
        # Simple span generation
        words = text.split()
        spans = []
        
        for i, word in enumerate(words[:10]):  # Limit to 10 spans for performance
            start = text.find(word)
            end = start + len(word)
            
            spans.append({
                "span": [start, end],
                "text": word,
                "scores": {
                    "V": round(random.random(), 3),
                    "A": round(random.random(), 3), 
                    "C": round(random.random(), 3)
                },
                "uncertainty": round(random.random(), 3)
            })
        
        return {
            "evaluations": {
                "short": spans[:3] if len(spans) > 3 else spans,
                "medium": spans[3:7] if len(spans) > 7 else spans,
                "long": spans[7:10] if len(spans) > 10 else spans,
                "stochastic": spans[-3:] if len(spans) > 3 else spans
            },
            "overallGrades": {
                "short": f"A{random.choice(['+', '', '-'])}",
                "medium": f"B{random.choice(['+', '', '-'])}",
                "long": f"C{random.choice(['+', '', '-'])}",
                "stochastic": f"B{random.choice(['+', '', '-'])}"
            },
            "textLength": len(text),
            "originalEvaluation": {
                "dataset_source": "mock_ethical_engine_v1.1",
                "processing_time": round(random.uniform(0.01, 0.1), 3)
            }
        }
    
    def structure_heat_map_data(text: str, evaluation_result: Dict[str, Any]) -> Dict[str, Any]:
        """Structure evaluation data for heat-map visualization."""
        # Convert full evaluation to heat-map format
        spans = evaluation_result.get("spans", [])
        
        # Categorize spans by type (this is a simplified version)
        heat_map_spans = []
        for span in spans:
            heat_map_spans.append({
                "span": [span.get("start", 0), span.get("end", 0)],
                "text": span.get("text", ""),
                "scores": {
                    "V": span.get("virtue_score", 0.5),
                    "A": span.get("deontological_score", 0.5),
                    "C": span.get("consequentialist_score", 0.5)
                },
                "uncertainty": span.get("uncertainty", 0.5)
            })
        
        return {
            "evaluations": {
                "short": heat_map_spans[:len(heat_map_spans)//4] if heat_map_spans else [],
                "medium": heat_map_spans[len(heat_map_spans)//4:len(heat_map_spans)//2] if heat_map_spans else [],
                "long": heat_map_spans[len(heat_map_spans)//2:3*len(heat_map_spans)//4] if heat_map_spans else [],
                "stochastic": heat_map_spans[3*len(heat_map_spans)//4:] if heat_map_spans else []
            },
            "overallGrades": evaluation_result.get("overallGrades", {"short": "B", "medium": "B", "long": "B", "stochastic": "B"}),
            "textLength": len(text),
            "originalEvaluation": {
                "dataset_source": "integrated_ethical_engine_v1.1",
                "processing_time": evaluation_result.get("processing_time", 0.0),
                "optimization_used": OPTIMIZED_COMPONENTS_AVAILABLE
            }
        }
    
    # Include API router
    app.include_router(api_router)
    
    # Application startup event
    @app.on_event("startup")
    async def startup_event():
        """Initialize integrated evaluation systems on startup."""
        logger.info("ðŸš€ Starting Ethical AI Developer Testbed - Integrated v1.1.0")
        await init_evaluation_systems()
        logger.info("âœ… Integrated evaluation systems ready")
    
    @app.on_event("shutdown") 
    async def shutdown_event():
        """Clean up resources on shutdown."""
        logger.info("ðŸ›‘ Shutting down Ethical AI Developer Testbed - Integrated")
        
        if OPTIMIZED_COMPONENTS_AVAILABLE:
            global_optimized_engine.cleanup()
            global_embedding_service.cleanup()
            global_cache_manager.clear_all_caches()
        
        executor.shutdown(wait=True)
        logger.info("âœ… Cleanup completed")
    
    return app

# Create the integrated application
app = create_integrated_app()

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)