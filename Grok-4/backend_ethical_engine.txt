# File: /app/backend/ethical_engine.py
"""
Ethical AI Evaluation Engine - Production Release

This module implements the core mathematical framework for multi-perspective ethical 
text evaluation with advanced dynamic scaling and machine learning capabilities.

Key Components:
- EthicalEvaluator: Main evaluation engine with sentence transformer integration
- LearningLayer: Machine learning system with dopamine-based feedback
- Dynamic Scaling: Adaptive threshold adjustment based on text complexity
- Multi-Perspective Analysis: Virtue, deontological, and consequentialist ethics

Mathematical Framework:
- Embedding Model: sentence-transformers/all-MiniLM-L6-v2
- Similarity Calculation: Cosine similarity with normalized vectors
- Threshold Application: Configurable per-perspective thresholds
- Span Detection: Evaluates 1-5 token spans for precise violation detection

Performance Features:
- Embedding caching for 2500x+ speedup on repeated evaluations
- Efficient span evaluation with optimized token processing
- Cascade filtering for fast obvious case detection
- Learning-based threshold optimization

Author: AI Developer Testbed Team
Version: 1.0 - Production Release
"""

import numpy as np
import re
import time
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass, field
from datetime import datetime
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def exponential_threshold_scaling(slider_value: float) -> float:
    """
    Convert 0-1 slider value to exponential threshold with enhanced granularity.
    
    This function provides fine-grained control in the critical 0.0-0.2 range
    where most ethical sensitivity adjustments occur. Uses exponential scaling
    to provide 28.9x better granularity compared to linear scaling.
    
    Args:
        slider_value (float): Input value from 0.0 to 1.0
        
    Returns:
        float: Exponentially scaled threshold value (0.0 to 0.5)
        
    Mathematical Formula:
        (e^(6*x) - 1) / (e^6 - 1) * 0.5
    """
    if slider_value <= 0:
        return 0.0
    if slider_value >= 1:
        return 0.5  # Increased max range from 0.3 to 0.5 for better distribution
    
    # Enhanced exponential function: e^(6*x) - 1 gives us range 0-0.5 with maximum granularity at bottom
    # This provides much finer control in the critical 0.0-0.2 range
    return (np.exp(6 * slider_value) - 1) / (np.exp(6) - 1) * 0.5

def linear_threshold_scaling(slider_value: float) -> float:
    """
    Convert 0-1 slider value to linear threshold with extended range.
    
    Simple linear scaling for comparison with exponential scaling.
    Provides uniform distribution across the full range.
    
    Args:
        slider_value (float): Input value from 0.0 to 1.0
        
    Returns:
        float: Linearly scaled threshold value (0.0 to 0.5)
    """
    return slider_value * 0.5  # Extended range to match exponential scaling

@dataclass
class LearningEntry:
    """Entry for learning system with dopamine feedback"""
    evaluation_id: str
    text_pattern: str
    ambiguity_score: float
    original_thresholds: Dict[str, float]
    adjusted_thresholds: Dict[str, float]
    feedback_score: float = 0.0
    feedback_count: int = 0
    created_at: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for MongoDB storage"""
        return {
            'evaluation_id': self.evaluation_id,
            'text_pattern': self.text_pattern,
            'ambiguity_score': self.ambiguity_score,
            'original_thresholds': self.original_thresholds,
            'adjusted_thresholds': self.adjusted_thresholds,
            'feedback_score': self.feedback_score,
            'feedback_count': self.feedback_count,
            'created_at': self.created_at
        }

@dataclass
class DynamicScalingResult:
    """Result of dynamic scaling process"""
    used_dynamic_scaling: bool
    used_cascade_filtering: bool
    ambiguity_score: float
    original_thresholds: Dict[str, float]
    adjusted_thresholds: Dict[str, float]
    processing_stages: List[str]
    cascade_result: Optional[str] = None  # "ethical", "unethical", or None

@dataclass
class EthicalParameters:
    """Configuration parameters for ethical evaluation"""
    # Thresholds for each perspective (Ï„_P) - Optimized for granular sensitivity
    virtue_threshold: float = 0.15  # Fine-tuned for better granularity
    deontological_threshold: float = 0.15  # Fine-tuned for better granularity
    consequentialist_threshold: float = 0.15  # Fine-tuned for better granularity
    
    # Vector magnitudes for ethical axes
    virtue_weight: float = 1.0
    deontological_weight: float = 1.0
    consequentialist_weight: float = 1.0
    
    # Span detection parameters (optimized for performance)
    max_span_length: int = 5  # Reduced from 10 for better performance
    min_span_length: int = 1
    
    # Model parameters
    embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2"
    
    # Dynamic scaling parameters
    enable_dynamic_scaling: bool = False
    enable_cascade_filtering: bool = False
    enable_learning_mode: bool = False
    exponential_scaling: bool = True
    
    # Cascade filtering thresholds - fine-tuned for better accuracy
    cascade_high_threshold: float = 0.25  # Adjusted for better granular range
    cascade_low_threshold: float = 0.08   # Lower for more granular detection
    
    # Learning parameters
    learning_weight: float = 0.3
    min_learning_samples: int = 5
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert parameters to dictionary for API responses"""
        return {
            'virtue_threshold': self.virtue_threshold,
            'deontological_threshold': self.deontological_threshold,
            'consequentialist_threshold': self.consequentialist_threshold,
            'virtue_weight': self.virtue_weight,
            'deontological_weight': self.deontological_weight,
            'consequentialist_weight': self.consequentialist_weight,
            'max_span_length': self.max_span_length,
            'min_span_length': self.min_span_length,
            'embedding_model': self.embedding_model,
            'enable_dynamic_scaling': self.enable_dynamic_scaling,
            'enable_cascade_filtering': self.enable_cascade_filtering,
            'enable_learning_mode': self.enable_learning_mode,
            'exponential_scaling': self.exponential_scaling,
            'cascade_high_threshold': self.cascade_high_threshold,
            'cascade_low_threshold': self.cascade_low_threshold,
            'learning_weight': self.learning_weight,
            'min_learning_samples': self.min_learning_samples
        }

@dataclass
class EthicalSpan:
    """Represents a span of text with ethical evaluation"""
    start: int
    end: int
    text: str
    virtue_score: float
    deontological_score: float
    consequentialist_score: float
    virtue_violation: bool
    deontological_violation: bool
    consequentialist_violation: bool
    is_minimal: bool = False
    
    @property
    def any_violation(self) -> bool:
        """Check if any perspective flags this span as unethical"""
        return self.virtue_violation or self.deontological_violation or self.consequentialist_violation
    
    @property
    def violation_perspectives(self) -> List[str]:
        """Return list of perspectives that flag this span"""
        perspectives = []
        if self.virtue_violation:
            perspectives.append("virtue")
        if self.deontological_violation:
            perspectives.append("deontological")
        if self.consequentialist_violation:
            perspectives.append("consequentialist")
        return perspectives
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert span to dictionary for API responses"""
        return {
            'start': int(self.start),
            'end': int(self.end),
            'text': str(self.text),
            'virtue_score': float(self.virtue_score),
            'deontological_score': float(self.deontological_score),
            'consequentialist_score': float(self.consequentialist_score),
            'virtue_violation': bool(self.virtue_violation),
            'deontological_violation': bool(self.deontological_violation),
            'consequentialist_violation': bool(self.consequentialist_violation),
            'any_violation': bool(self.any_violation),
            'violation_perspectives': self.violation_perspectives,
            'is_minimal': bool(self.is_minimal)
        }

@dataclass
class EthicalEvaluation:
    """Complete ethical evaluation result"""
    input_text: str
    tokens: List[str]
    spans: List[EthicalSpan]
    minimal_spans: List[EthicalSpan]
    overall_ethical: bool
    processing_time: float
    parameters: EthicalParameters
    dynamic_scaling_result: Optional[DynamicScalingResult] = None
    evaluation_id: str = field(default_factory=lambda: f"eval_{int(time.time() * 1000)}")
    
    @property
    def violation_count(self) -> int:
        """Count of spans with violations"""
        return len([s for s in self.spans if s.any_violation])
    
    @property
    def minimal_violation_count(self) -> int:
        """Count of minimal spans with violations"""
        return len([s for s in self.minimal_spans if s.any_violation])
    
    @property
    def all_spans_with_scores(self) -> List[Dict[str, Any]]:
        """All spans with their scores for detailed analysis"""
        return [s.to_dict() for s in self.spans]
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert evaluation to dictionary for API responses"""
        result = {
            'evaluation_id': str(self.evaluation_id),
            'input_text': str(self.input_text),
            'tokens': [str(token) for token in self.tokens],
            'spans': [s.to_dict() for s in self.spans],
            'minimal_spans': [s.to_dict() for s in self.minimal_spans],
            'all_spans_with_scores': self.all_spans_with_scores,
            'overall_ethical': bool(self.overall_ethical),
            'processing_time': float(self.processing_time),
            'violation_count': int(self.violation_count),
            'minimal_violation_count': int(self.minimal_violation_count),
            'parameters': self.parameters.to_dict()
        }
        
        if self.dynamic_scaling_result:
            result['dynamic_scaling'] = {
                'used_dynamic_scaling': bool(self.dynamic_scaling_result.used_dynamic_scaling),
                'used_cascade_filtering': bool(self.dynamic_scaling_result.used_cascade_filtering),
                'ambiguity_score': float(self.dynamic_scaling_result.ambiguity_score),
                'original_thresholds': {k: float(v) for k, v in self.dynamic_scaling_result.original_thresholds.items()},
                'adjusted_thresholds': {k: float(v) for k, v in self.dynamic_scaling_result.adjusted_thresholds.items()},
                'processing_stages': [str(stage) for stage in self.dynamic_scaling_result.processing_stages],
                'cascade_result': str(self.dynamic_scaling_result.cascade_result) if self.dynamic_scaling_result.cascade_result else None
            }
        
        return result

# Rest of the file continues with the actual implementation classes...
# This is just the first part to show the structure to Grok!